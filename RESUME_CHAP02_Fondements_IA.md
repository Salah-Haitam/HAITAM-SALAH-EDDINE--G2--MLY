# üìö R√âSUM√â - Chapitre 02 : Fondements de l'Intelligence Artificielle
**Dr. Ghezlane Halhoul Merabet | EMSI 2024-2025**

---

## üéØ PARTIE 1 : R√âSOLUTION DE PROBL√àMES EN IA

### Le Probl√®me : L'Explosion Combinatoire
- Un ordinateur ne peut pas "tout essayer" ‚Üí trop de possibilit√©s
- **Exemple TSP** (Voyageur de Commerce) : 20 villes = **1 milliard de milliards** de trajets possibles ‚Üí 38 000 ans de calcul !

### Les Solutions :

| M√©thode | D√©finition | Avantages | Inconv√©nients |
|---------|------------|-----------|---------------|
| **Heuristique** | R√®gle approximative "cherche par l√†" | Tr√®s rapide | Pas toujours optimal |
| **M√©taheuristique** | Strat√©gie qui dirige les heuristiques | Trouve le vrai meilleur | Plus complexe |

**Cl√© des m√©taheuristiques** : Accepter temporairement des solutions MOINS bonnes pour explorer mieux

**√âquilibre** :
- **Exploration** = chercher partout (d√©but)
- **Exploitation** = approfondir les zones prometteuses (fin)

---

## üéØ PARTIE 2 : MACHINE LEARNING (ML)

### D√©finitions √† retenir

> **Intuitive (Samuel, 1959)** : La capacit√© d'une machine √† apprendre SANS √™tre explicitement programm√©e

> **Formelle (Mitchell, 1997)** : Un syst√®me apprend de l'**Exp√©rience E** par rapport √† une **T√¢che T** et une **Performance P**, si sa performance s'am√©liore avec l'exp√©rience

### Exemple concret - D√©tection Spam :
- **T** (T√¢che) = Classifier emails Spam/Non-Spam
- **E** (Exp√©rience) = Analyse d'emails d√©j√† classifi√©s
- **P** (Performance) = % d'emails correctement classifi√©s

### Programmation Traditionnelle vs ML

| Traditionnelle | Machine Learning |
|----------------|------------------|
| Donn√©es + R√®gles ‚Üí R√©sultats | Donn√©es + R√©sultats ‚Üí R√®gles |
| R√®gles manuelles (IF-THEN) | R√®gles d√©couvertes automatiquement |
| Rigide | S'adapte |

### Les 3 Composantes Cl√©s du ML

```
DONN√âES (Carburant) ‚Üí MOD√àLES (Cerveau) ‚Üí APPRENTISSAGE (Entra√Ænement)
```

---

## üéØ PARTIE 3 : LES DONN√âES

### Types de donn√©es :

| Type | Organisation | Exemples |
|------|--------------|----------|
| **Structur√©es** | Tableaux, BDD | √Çge, revenu, temp√©rature |
| **Semi-structur√©es** | Balises/marqueurs | XML, JSON, logs, emails |
| **Non-structur√©es** | Format libre | Images, textes, audio |

### Cycle de vie des donn√©es :
1. **Collecte** ‚Üí Sources fiables, m√©thodes appropri√©es, √©thique
2. **Pr√©paration** ‚Üí Nettoyage, transformation, augmentation
3. **Utilisation** ‚Üí Train/Test, validation, mise √† jour

---

## üéØ PARTIE 4 : LES MOD√àLES

### Types de mod√®les :

| Mod√®le | Objectif | Exemple |
|--------|----------|---------|
| **R√©gression** | Pr√©dire valeurs continues | Prix maison : Y = aX + b |
| **Classification** | Cat√©goriser | Spam/Non-spam (KNN) |
| **Clustering** | Regrouper sans √©tiquettes | Segmentation clients (K-Means) |

---

## üéØ PARTIE 5 : TYPES D'APPRENTISSAGE

### 1Ô∏è‚É£ Apprentissage SUPERVIS√â
- **Donn√©es √©tiquet√©es** (entr√©es + sorties attendues)
- Apprend par correction d'erreurs
- *Applications* : R√©gression, Classification

### 2Ô∏è‚É£ Apprentissage NON-SUPERVIS√â
- **Pas d'√©tiquettes**
- D√©couverte automatique de patterns
- *Applications* : Clustering, R√©duction de dimensionnalit√©

### 3Ô∏è‚É£ Apprentissage par RENFORCEMENT
- **Essai-erreur** avec r√©compenses/p√©nalit√©s
- Optimisation √† long terme
- *Exemple* : Agent dans un labyrinthe

---

## üéØ PARTIE 6 : DEEP LEARNING

### D√©finition
> Branche du ML utilisant des **r√©seaux de neurones multicouches** pour apprendre des motifs de plus en plus complexes

### Hi√©rarchie : IA ‚äÉ ML ‚äÉ Deep Learning

### 4 Aspects Distinctifs :

1. **Extraction automatique des features** ‚Üí Plus besoin de d√©finir manuellement les caract√©ristiques
2. **Apprentissage hi√©rarchique** ‚Üí Du simple au complexe (caract√®res ‚Üí mots ‚Üí phrases ‚Üí sens)
3. **Apprentissage bout-en-bout** ‚Üí Pas d'√©tapes interm√©diaires manuelles
4. **Repr√©sentations distribu√©es** ‚Üí Un concept = plusieurs neurones, un neurone = plusieurs concepts

---

## üéØ PARTIE 7 : R√âSEAUX DE NEURONES

### Architecture de base :

```
[Couche d'Entr√©e] ‚Üí [Couches Cach√©es] ‚Üí [Couche de Sortie]
   (donn√©es)         (traitement)         (pr√©diction)
```

### Fonctionnement d'un neurone :

```
Entr√©es (X‚ÇÅ, X‚ÇÇ, ..., X‚Çô) √ó Poids (w) + Biais (b) ‚Üí Fonction d'activation ‚Üí Sortie (Y)
```

**Formule** : Y = f(Œ£ w·µ¢X·µ¢ + b)

### Le Biais (b) :
- Ajustement qui d√©place la courbe de d√©cision
- Sans biais : toutes les courbes passent par l'origine ‚Üí limite le mod√®le

### Fonctions d'activation :

| Fonction | Plage | Usage |
|----------|-------|-------|
| **Sigmo√Øde** | [0, 1] | Classification binaire |
| **ReLU** | [0, +‚àû] | CNN, r√©seaux profonds |

### Processus d'apprentissage :

1. **Propagation avant** ‚Üí Donn√©es traversent le r√©seau ‚Üí Pr√©diction
2. **Fonction de co√ªt** ‚Üí Mesure l'erreur : C = ¬Ω(≈∂ - Y)¬≤
3. **R√©tropropagation** ‚Üí Ajuste les poids pour minimiser l'erreur
4. **R√©p√©ter** jusqu'√† convergence

---

## üéØ PARTIE 8 : VISION PAR ORDINATEUR (Computer Vision)

### Ce que voit l'ordinateur :
- Image = Matrice de nombres [0-255]
- Image RGB 1080√ó1080 = matrice de **1080 √ó 1080 √ó 3** valeurs

### CNN (Convolutional Neural Network) :
Architecture sp√©cialis√©e pour les images

**Structure** :
```
Image ‚Üí Convolution (d√©tecte motifs) ‚Üí Pooling (r√©duit taille) ‚Üí Fully Connected ‚Üí Sortie
```

**Apprentissage hi√©rarchique** :
- Niveau bas : contours, bordures
- Niveau moyen : formes, textures
- Niveau haut : objets complets

---

## üéØ PARTIE 9 : NLP (Traitement du Langage Naturel)

### D√©finition
> Branche de l'IA permettant aux machines de comprendre et g√©n√©rer le langage humain

### √âtape 1 : Tokenization (d√©coupage du texte)

| Type | Exemple |
|------|---------|
| **Par mots** | "L'IA apprend" ‚Üí ["L'", "IA", "apprend"] |
| **Par sous-mots** | "pr√©traitement" ‚Üí ["pr√©", "##traite", "##ment"] |
| **Par caract√®res** | "Hello" ‚Üí ["H", "e", "l", "l", "o"] |

### √âtape 2 : Word Embeddings (vecteurs)
- Chaque mot = vecteur de 100-300 dimensions
- Mots similaires ‚Üí vecteurs proches
- Capture les relations : roi - homme + femme ‚âà reine

### Application : Analyse de sentiments
Texte ‚Üí Pr√©traitement ‚Üí Extraction features ‚Üí Classification (positif/n√©gatif/neutre)

---

## üìù FORMULES ESSENTIELLES √Ä RETENIR

| Concept | Formule |
|---------|---------|
| R√©gression lin√©aire | Y = aX + b |
| Somme pond√©r√©e | Œ£ w·µ¢X·µ¢ + b |
| Fonction de co√ªt | C = ¬Ω(≈∂ - Y)¬≤ |

---

## üîë MOTS-CL√âS POUR L'EXAMEN

- Explosion combinatoire, Heuristique, M√©taheuristique
- Exploration vs Exploitation
- Supervis√©, Non-supervis√©, Renforcement
- Features, Extraction automatique
- Propagation avant, R√©tropropagation
- Sigmo√Øde, ReLU
- CNN, Convolution, Pooling
- Tokenization, Word Embeddings

---

## ‚úÖ CHECKLIST DE R√âVISION

- [ ] Je sais expliquer pourquoi on ne peut pas "tout essayer"
- [ ] Je connais la diff√©rence heuristique/m√©taheuristique
- [ ] Je ma√Ætrise les 3 types d'apprentissage (supervis√©, non-supervis√©, renforcement)
- [ ] Je sais comment fonctionne un neurone artificiel
- [ ] Je comprends le r√¥le de la fonction d'activation
- [ ] Je connais le processus : propagation avant ‚Üí co√ªt ‚Üí r√©tropropagation
- [ ] Je sais ce qu'est un CNN et son utilit√©
- [ ] Je comprends la tokenization et les embeddings en NLP

---

*R√©sum√© g√©n√©r√© √† partir du cours CHAP-02 Fondements-AI.pdf (84 pages)*
